{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(tokens):\n",
    "    '''\n",
    "    Remove stopwords;\n",
    "\n",
    "    \n",
    "    Remove punctuation\n",
    "    '''\n",
    "    stop_words = stopwords.words('english')\n",
    "    res = [w.lower() for w in tokens if not w in stop_words] ## Remove stopwords\n",
    "    res_punc = [w for w in res if w.isalpha()] ## Remove punctuation\n",
    "    return res_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word_tokenize(x) for x in train]\n",
    "clean_tokens = [clean_descriptions(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() ## numbers of cores in the computer\n",
    "model = Word2Vec(min_count=1,\n",
    "                window=5,\n",
    "                size=50,\n",
    "                sample=6e-5,\n",
    "                alpha=0.03,\n",
    "                min_alpha=0.0007,\n",
    "                negative=20,\n",
    "                workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to train the model: 0.6256 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "model.train(clean_tokens, total_examples=model.corpus_count, epochs=100)\n",
    "print(f\"Time to train the model: {round((time() - t) / 60, 4)} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('barack', 0.7298361659049988),\n",
       " ('donald', 0.6451386213302612),\n",
       " ('trump', 0.642876148223877),\n",
       " ('administration', 0.6392666697502136),\n",
       " ('president', 0.6343703866004944),\n",
       " ('environmental', 0.5987725257873535),\n",
       " ('congress', 0.5691197514533997),\n",
       " ('deliver', 0.5623358488082886),\n",
       " ('policy', 0.5582756996154785),\n",
       " ('immigration', 0.5485448241233826)]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "''' Checking for similar words '''\n",
    "model.wv.most_similar(['obama'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/word2vec_model.model')"
   ]
  },
  {
   "source": [
    "### Load test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "source": [
    "### Prepare test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For a batter visualization\n",
    "we're going to use 100 random headlines\n",
    "'''\n",
    "test = random.sample(test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word_tokenize(x) for x in test]\n",
    "clean_test_data = [clean_descriptions(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word unedited not found in vocab\nWord hashimoto not found in vocab\nWord illusions not found in vocab\nWord mantel not found in vocab\nWord romanticizing not found in vocab\nWord boone not found in vocab\nWord drunkenly not found in vocab\nWord durango not found in vocab\nWord gustave not found in vocab\nWord sats not found in vocab\nWord patriarch not found in vocab\nWord weaponizes not found in vocab\n"
     ]
    }
   ],
   "source": [
    "words, vectors = [], []\n",
    "for desc in clean_test_data: ## Iterate over descriptions in test data\n",
    "    for word in desc: ## Iterate over words in descriptions\n",
    "        try:\n",
    "            vectors.append(model.wv.get_vector(word))\n",
    "            words.append(word)\n",
    "        except KeyError:\n",
    "            print(f'Word {word} not found in vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "883"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data/words.txt', words, fmt='%s')\n",
    "np.save('data/vectors.npy', vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}